{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Consistency Prompting\n",
    "\n",
    "One of the more advanced techniques in prompt engineering is self-consistency, introduced by `Wang et al. (2022)`. \n",
    "\n",
    "This method seeks to improve upon the traditional greedy decoding typically used in chain-of-thought (CoT) prompting. \n",
    "\n",
    "The core concept involves sampling multiple diverse reasoning paths through few-shot CoT and leveraging these variations to determine the most consistent answer. The technique  enhances the effectiveness of CoT prompting, particularly for tasks requiring arithmetic and commonsense reasoning.\n",
    "\n",
    "## References:\n",
    "* [Wang et al. (2022)](https://arxiv.org/abs/2203.11171)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running this code on MyBind.org\n",
    "\n",
    "Note: remember that you will need to **adjust CONFIG** with **proper URL and API_KEY**!\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/GenILab-FAU/prompt-eng/HEAD?urlpath=%2Fdoc%2Ftree%2Fprompt-eng%2Fself_consistency.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To calculate this, we need to know that the logarithm of 2 (base 10) is approximately 0.301.\n",
      "\n",
      "So,\n",
      "\n",
      "984 * log(2) ≈ 984 * 0.301\n",
      "≈ 295.584\n",
      "Time taken: 5.399s\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## SELF-CONSISTENCY PROMPTING FOR STUDY COMPANION BOT\n",
    "##\n",
    "\n",
    "from _pipeline import create_payload, model_req\n",
    "\n",
    "#### (1) Adjust the inbounding Prompt, simulating inbounding requests from users or other systems\n",
    "MESSAGE = \"Extract functional and non-functional requirements for a Discord-based Study Companion Bot.\"\n",
    "\n",
    "#### (2) Adjust the Prompt Engineering Technique to be applied, simulating Workflow Templates\n",
    "SELF_CONSISTENCY = \"\"\"You are a software analyst extracting requirements for AI-powered educational chatbots.\n",
    "To ensure consistency in requirement extraction, analyze the input multiple times and cross-check the outputs before providing the final response.\n",
    "\n",
    "Attempt 1:\n",
    "Input: \"Extract software requirements for a Study Companion Bot.\"\n",
    "Output:\n",
    "Functional Requirements:\n",
    "- The bot should provide course management and learning path suggestions.\n",
    "- It should fetch answers to FAQs using APIs like Wikipedia and DuckDuckGo.\n",
    "- The bot should generate AI-based explanations for concepts like Agile, AI, and Data Structures.\n",
    "- It should retrieve subject concepts and learning resources from sources like W3Schools, MDN Web Docs, and Stack Overflow.\n",
    "- The bot should set study reminders and schedules using Google Calendar API.\n",
    "- It should facilitate study group collaboration through discussion channels and quizzes.\n",
    "\n",
    "Non-Functional Requirements:\n",
    "- The bot should respond to queries within 2 seconds.\n",
    "- It should handle at least 500 concurrent users efficiently.\n",
    "- The system should have a 99.9% uptime.\n",
    "- User data and interactions should be encrypted to ensure privacy.\n",
    "- The bot should be scalable to support additional study modules.\n",
    "\n",
    "Attempt 2:\n",
    "Input: \"Extract software requirements for a Study Companion Bot.\"\n",
    "Output:\n",
    "Functional Requirements:\n",
    "- The bot should suggest personalized study plans based on user progress.\n",
    "- It should answer frequently asked questions using AI-generated responses.\n",
    "- The bot should integrate with external educational resources like W3Schools, MDN, and Stack Overflow.\n",
    "- It should support productivity techniques like Pomodoro timers and scheduling via Google Calendar API.\n",
    "- The bot should enable peer-to-peer learning by facilitating study groups and topic-based discussions.\n",
    "\n",
    "Non-Functional Requirements:\n",
    "- The response time should be less than 2 seconds for general queries.\n",
    "- The bot should support at least 500+ active users at a time.\n",
    "- All communications should be secured with end-to-end encryption.\n",
    "- The system should be modular and allow easy integration with additional APIs.\n",
    "\n",
    "Final Aggregated Output:\n",
    "Functional Requirements:\n",
    "- The bot should provide course management and personalized learning path suggestions.\n",
    "- It should answer FAQs using external APIs and AI-generated responses.\n",
    "- The bot should retrieve subject concepts and reference materials from external sources.\n",
    "- It should set study reminders, schedules, and integrate with Google Calendar API.\n",
    "- The bot should facilitate collaborative learning through discussion channels and quizzes.\n",
    "- The bot should support productivity techniques like Pomodoro timers.\n",
    "\n",
    "Non-Functional Requirements:\n",
    "- The bot should provide real-time responses (under 2 seconds).\n",
    "- It should handle at least 500+ active users efficiently.\n",
    "- The system should maintain 99.9% uptime.\n",
    "- All user data should be encrypted for security.\n",
    "- The architecture should be scalable for future enhancements.\n",
    "\n",
    "Now, given the following input, perform multiple consistency checks and extract the final software requirements:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = SELF_CONSISTENCY + '\\n' + MESSAGE \n",
    "\n",
    "#### (3) Configure the Model request, simulating Workflow Orchestration\n",
    "# Documentation: https://github.com/ollama/ollama/blob/main/docs/api.md\n",
    "payload = create_payload(target=\"ollama\",\n",
    "                         model=\"llama3.2:latest\", \n",
    "                         prompt=PROMPT, \n",
    "                         temperature=0.5, \n",
    "                         num_ctx=150, \n",
    "                         num_predict=1200)\n",
    "\n",
    "### YOU DONT NEED TO CONFIGURE ANYTHING ELSE FROM THIS POINT\n",
    "# Send out to the model\n",
    "time, response = model_req(payload=payload)\n",
    "print(response)\n",
    "if time: print(f'Time taken: {time}s')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
