{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain-of-Thought Prompting\n",
    "\n",
    "Chain-of-Thought (CoT) prompting enhances complex reasoning by encouraging the model to break down problems into intermediate reasoning steps. When combined with few-shot prompting, it can significantly improve performance on tasks that require multi-step reasoning before arriving at a response.\n",
    "\n",
    "## Automatic Chain-of-Thought (Auto-CoT)\n",
    "\n",
    "Traditionally, using CoT prompting with demonstrations involves manually crafting diverse and effective examples. This manual effort is time-consuming and can lead to less-than-optimal results. To address this, Zhang et al. (2022) introduced Auto-CoT, an automated approach that minimizes manual involvement. Their method uses the prompt “Let’s think step by step” to generate reasoning chains automatically for demonstrations. However, this automatic process is not immune to errors. To reduce the impact of such mistakes, the approach emphasizes the importance of diverse demonstrations.\n",
    "\n",
    "Auto-CoT operates in two main stages:\n",
    "\n",
    "1. **Question Clustering:** Questions from the dataset are grouped into clusters based on similarity or relevance.\n",
    "2. **Demonstration Sampling:** A representative question from each cluster is selected, and its reasoning chain is generated using Zero-Shot-CoT guided by simple heuristics.\n",
    "\n",
    "\n",
    "## References:\n",
    "\n",
    "* (Wei et al. (2022),)[https://arxiv.org/abs/2201.11903]\n",
    "* (OpenAI Documentation for Prompt Engineering)[https://platform.openai.com/docs/guides/prompt-engineering]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running this code on MyBind.org\n",
    "\n",
    "Note: remember that you will need to **adjust CONFIG** with **proper URL and API_KEY**!\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/GenILab-FAU/prompt-eng/HEAD?urlpath=%2Fdoc%2Ftree%2Fprompt-eng%2Fchain_of_thought.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'llama3.2:latest', 'prompt': '\\nGiven the following classification model evaluation metrics, determine the F1-score and evaluate its quality:\\n\\n1. Precision: 0.9, Recall: 0.8\\nA: F1-score = 2 * (Precision * Recall) / (Precision + Recall) = 2 * (0.9 * 0.8) / (0.9 + 0.8) = 0.84. The answer is False.\\n\\n2. Precision: 0.75, Recall: 0.75\\nA: F1-score = 2 * (Precision * Recall) / (Precision + Recall) = 2 * (0.75 * 0.75) / (0.75 + 0.75) = 0.75. The answer is False.\\n\\n3. Precision: 0.85, Recall: 0.85\\nA: F1-score = 2 * (Precision * Recall) / (Precision + Recall) = 2 * (0.85 * 0.85) / (0.85 + 0.85) = 0.85. The answer is True.\\n\\n4. Precision: 0.6, Recall: 0.9\\nA: F1-score = 2 * (Precision * Recall) / (Precision + Recall) = 2 * (0.6 * 0.9) / (0.6 + 0.9) = 0.72. The answer is False.\\n\\nNow calculate the F1-score for this model and let me know if it is \"0.85\".\\nProvide only the answer, no explanation!\\n', 'stream': False, 'options': {'temperature': 0.2, 'num_ctx': 20, 'num_predict': 2048}}\n",
      "3.14159\n",
      "Time taken: 4.202s\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "## CHAIN-OF-THOUGHT PROMPTING FOR SOFTWARE REQUIREMENT EXTRACTION\n",
    "##\n",
    "\n",
    "from _pipeline import create_payload, model_req\n",
    "\n",
    "#### (1) Adjust the inbounding Prompt, simulating inbounding requests from users or other systems\n",
    "MESSAGE = \"Extract software requirements for a Discord-based Study Companion Bot.\"\n",
    "\n",
    "#### (2) Adjust the Prompt Engineering Technique to be applied, simulating Workflow Templates\n",
    "CHAIN_OF_THOUGHT = \\\n",
    "f\"\"\"\n",
    "To systematically extract software requirements, follow these structured steps:\n",
    "\n",
    "1. Identify the **core functionalities** required for the bot, such as answering questions, managing study schedules, and facilitating collaboration.\n",
    "2. List the **functional requirements** that describe the bot's expected operations (e.g., \"The bot should fetch learning resources from external APIs\").\n",
    "3. Outline the **non-functional requirements** that specify quality attributes (e.g., \"The bot should provide responses within 2 seconds\").\n",
    "4. Organize the extracted requirements into **categories** like User Interaction, Data Handling, Security, and Performance.\n",
    "\n",
    "Now, generate a well-structured list of requirements for the given system.\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = CHAIN_OF_THOUGHT \n",
    "\n",
    "#### (3) Configure the Model request, simulating Workflow Orchestration\n",
    "# Documentation: https://github.com/ollama/ollama/blob/main/docs/api.md\n",
    "payload = create_payload(target=\"ollama\",\n",
    "                         model=\"llama3.2:latest\", \n",
    "                         prompt=PROMPT, \n",
    "                         temperature=0.3, \n",
    "                         num_ctx=100, \n",
    "                         num_predict=512)\n",
    "\n",
    "### YOU DONT NEED TO CONFIGURE ANYTHING ELSE FROM THIS POINT\n",
    "# Send out to the model\n",
    "time, response = model_req(payload=payload)\n",
    "print(response)\n",
    "if time: print(f'Time taken: {time}s')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
